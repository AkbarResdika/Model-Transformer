{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Install library databits\n",
        "\n",
        "source : https://pypi.org/project/databits/"
      ],
      "metadata": {
        "id": "0ndSe2YBjV33"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVxAu0USgC4K",
        "outputId": "449bd6bb-97a4-4843-ef8f-b846adfe04e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: databits in /usr/local/lib/python3.10/dist-packages (2.0.5)\n",
            "Requirement already satisfied: torchtext==0.17.0 in /usr/local/lib/python3.10/dist-packages (from databits) (0.17.0)\n",
            "Requirement already satisfied: bitsandbytes==0.40.2 in /usr/local/lib/python3.10/dist-packages (from databits) (0.40.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0->databits) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0->databits) (2.32.3)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0->databits) (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0->databits) (1.26.4)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext==0.17.0->databits) (0.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->torchtext==0.17.0->databits) (2.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext==0.17.0->databits) (2.2.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchtext==0.17.0->databits) (12.6.85)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0->databits) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0->databits) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.17.0->databits) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->torchtext==0.17.0->databits) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0->torchtext==0.17.0->databits) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install databits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Persiapan data"
      ],
      "metadata": {
        "id": "o2v1Xx3cjgEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import pandas as pd\n",
        "\n",
        "# Load the training data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "# Check column names (we should print this out to check what columns are available)\n",
        "print(f\"train_df columns: {train_df.columns}\")\n",
        "\n",
        "# Extract the correct column instead of 'text'\n",
        "X_train_list = train_df.iloc[:, 1].tolist()\n",
        "# Extract the correct column instead of 'label'\n",
        "y_train_list = train_df.iloc[:, 0].tolist()\n",
        "\n",
        "# Load the testing data\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "print(f\"test_df columns: {test_df.columns}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nJcK9dvZC-2",
        "outputId": "97171f7d-0290-4c31-c486-07b857ab8dca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df columns: Index(['3', 'Wall St. Bears Claw Back Into the Black (Reuters)',\n",
            "       'Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.'],\n",
            "      dtype='object')\n",
            "test_df columns: Index(['3', 'Fears for T N pension after talks',\n",
            "       'Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the correct column instead of 'text'\n",
        "X_test_list = test_df.iloc[:, 1].tolist()\n",
        "# Extract the correct column instead of 'label'\n",
        "y_test_list = test_df.iloc[:, 0].tolist()\n",
        "\n",
        "# Convert y_train and y_test to integers\n",
        "y_train_list = [int(label) for label in y_train_list]\n",
        "y_test_list = [int(label) for label in y_test_list]\n",
        "\n",
        "# Create DataFrames for X_train and y_train\n",
        "X_train_df = pd.DataFrame({'text': X_train_list})\n",
        "y_train_df = pd.DataFrame({'label': y_train_list})\n",
        "\n",
        "# Create DataFrames for X_test and y_test\n",
        "X_test_df = pd.DataFrame({'text': X_test_list})\n",
        "y_test_df = pd.DataFrame({'label': y_test_list})\n",
        "\n",
        "# Concatenate X_train and y_train into a single DataFrame\n",
        "train_data = pd.concat([y_train_df, X_train_df], axis=1)\n",
        "\n",
        "# Concatenate X_test and y_test into a single DataFrame\n",
        "test_data = pd.concat([y_test_df, X_test_df], axis=1)\n",
        "\n",
        "# Print the info\n",
        "print(\"X_train length:\", len(X_train_list))\n",
        "print(\"y_train length:\", len(y_train_list))\n",
        "print(\"X_test length:\", len(X_test_list))\n",
        "print(\"y_test length:\", len(y_test_list))"
      ],
      "metadata": {
        "id": "C38O2OqphEe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8763ee8e-f0b4-4e64-b5ca-f04a351b3029"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train length: 119999\n",
            "y_train length: 119999\n",
            "X_test length: 7599\n",
            "y_test length: 7599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few rows of the resulting DataFrames\n",
        "print(\"\\nTrain Data:\")\n",
        "print(train_data.head())\n",
        "print(\"\\nTest Data:\")\n",
        "print(test_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2BkDaIfZxuw",
        "outputId": "096d7216-a7fa-4c41-cfe6-49f95fae654f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Data:\n",
            "   label                                               text\n",
            "0      3  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
            "1      3    Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
            "2      3  Iraq Halts Oil Exports from Main Southern Pipe...\n",
            "3      3  Oil prices soar to all-time record, posing new...\n",
            "4      3        Stocks End Up, But Near Year Lows (Reuters)\n",
            "\n",
            "Test Data:\n",
            "   label                                               text\n",
            "0      4  The Race is On: Second Private Team Sets Launc...\n",
            "1      4      Ky. Company Wins Grant to Study Peptides (AP)\n",
            "2      4      Prediction Unit Helps Forecast Wildfires (AP)\n",
            "3      4        Calif. Aims to Limit Farm-Related Smog (AP)\n",
            "4      4  Open Letter Against British Copyright Indoctri...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Preprocessing Data"
      ],
      "metadata": {
        "id": "koX6rFLbZ_rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJN_MhwYaGLt",
        "outputId": "98f7f797-c82c-4190-b4c8-9eaabc72d1f7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v12VpNWaNtg",
        "outputId": "ef1f59b5-d5fa-49e5-e7ee-96ba3ff6aae5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m5.1/6.8 MB\u001b[0m \u001b[31m153.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m135.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=ea9adc4e9a0d56f456b4fad0552dc4774efa1940f2d99e7fcef64ac8e477c469\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk import pos_tag\n",
        "from spellchecker import SpellChecker\n",
        "import spacy\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "# Download the 'punkt_tab' data package\n",
        "nltk.download('punkt_tab')\n",
        "# Download the missing 'averaged_perceptron_tagger_eng' data package\n",
        "nltk.download('averaged_perceptron_tagger_eng') # This line was added to download the missing package.\n",
        "\n",
        "\n",
        "# Load Spacy model for entity recognition\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Initialize tools\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Define stopwords and other constants\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # POS tagging\n",
        "    tokens_with_pos = pos_tag(tokens)\n",
        "\n",
        "    # Stopword elimination and POS-based stopword elimination\n",
        "    tokens = [word for word, pos in tokens_with_pos if word not in stop_words and pos not in ['PRP', 'PRP$', 'IN']]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Rejoin tokens into a single string\n",
        "    processed_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Apply preprocessing to the train and test datasets\n",
        "train_data['processed_text'] = train_data['text'].apply(preprocess_text)\n",
        "test_data['processed_text'] = test_data['text'].apply(preprocess_text)\n",
        "\n",
        "# Check the results\n",
        "print(\"\\nProcessed Train Data:\")\n",
        "print(train_data[['label', 'processed_text']].head())\n",
        "print(\"\\nProcessed Test Data:\")\n",
        "print(test_data[['label', 'processed_text']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1GcKroYaCF0",
        "outputId": "4924cf35-0c35-4a7e-bfb6-b97b5e625abe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed Train Data:\n",
            "   label                                     processed_text\n",
            "0      3      carlyle look commercial aerospace ( reuters )\n",
            "1      3      oil economy cloud stock ' outlook ( reuters )\n",
            "2      3  iraq halt oil export main southern pipeline ( ...\n",
            "3      3  oil price soar all-time record , posing new me...\n",
            "4      3                   stock end , year low ( reuters )\n",
            "\n",
            "Processed Test Data:\n",
            "   label                                     processed_text\n",
            "0      4  race : second private team set launch date hum...\n",
            "1      4         ky. company win grant study peptide ( ap )\n",
            "2      4      prediction unit help forecast wildfire ( ap )\n",
            "3      4          calif. aim limit farm-related smog ( ap )\n",
            "4      4  open letter british copyright indoctrination s...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Definisikan hyperparameter"
      ],
      "metadata": {
        "id": "S7Sgf_cJjmQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from databits import CreateModel\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "SEQUENCE_LENGTH = 100\n",
        "EPOCHS = 5\n",
        "EMBED_DIM = 512\n",
        "N_LAYERS = 2\n",
        "DROPOUT_RATE = 0.1\n",
        "NUM_CLASSES = len(np.unique(np.array(y_train)))\n",
        "OPTIMIZER = torch.optim.Adam\n",
        "LR = 0.001\n",
        "LOSS = nn.CrossEntropyLoss"
      ],
      "metadata": {
        "id": "v0jRnmQDiACZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Bangun Model"
      ],
      "metadata": {
        "id": "OorL9ghfjpMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CreateModel(X_train, y_train,\n",
        "                 X_test, y_test,\n",
        "                 batch=BATCH_SIZE,\n",
        "                 seq=SEQUENCE_LENGTH,\n",
        "                 embedding_dim=EMBED_DIM,\n",
        "                 n_layers=N_LAYERS,\n",
        "                 dropout_rate=DROPOUT_RATE,\n",
        "                 num_classes=NUM_CLASSES)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIMyBtH1iFSw",
        "outputId": "e8e5f926-d0d4-4afe-b0be-3bb9f0758fbb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading setup data ...\n",
            "Loading train data ...\n",
            "Loading val data ...\n",
            "Successful load model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Latih model Fasttext"
      ],
      "metadata": {
        "id": "UmTa7K1cjsxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.FASTTEXT() # fasttext model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-C60l9liIqc",
        "outputId": "bd80a72d-b458-431c-ce30-2b2a10bf88e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FASTTEXTModel(\n",
              "  (embedding): Embedding(98639, 512)\n",
              "  (layers): ModuleList(\n",
              "    (0-1): 2 x Linear(in_features=512, out_features=512, bias=True)\n",
              "  )\n",
              "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(epochs=EPOCHS, optimizer=OPTIMIZER, lr=LR, loss=LOSS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe81mIaMiTjV",
        "outputId": "b018892f-e150-4b86-856a-362702b9e0bc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 3750/3750 [01:24<00:00, 44.42batch/s]\n",
            "Validation: 100%|██████████| 238/238 [00:00<00:00, 300.82batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Train Loss: 0.4793 | Train Acc: 0.8126 | Val Loss: 0.3006 | Val Acc: 0.9021\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 3750/3750 [01:23<00:00, 44.68batch/s]\n",
            "Validation: 100%|██████████| 238/238 [00:00<00:00, 315.91batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5 | Train Loss: 0.2511 | Train Acc: 0.9151 | Val Loss: 0.2543 | Val Acc: 0.9129\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 3750/3750 [01:23<00:00, 44.96batch/s]\n",
            "Validation: 100%|██████████| 238/238 [00:01<00:00, 226.51batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5 | Train Loss: 0.1917 | Train Acc: 0.9359 | Val Loss: 0.2687 | Val Acc: 0.9101\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 3750/3750 [01:27<00:00, 43.03batch/s]\n",
            "Validation: 100%|██████████| 238/238 [00:00<00:00, 331.17batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5 | Train Loss: 0.1569 | Train Acc: 0.9470 | Val Loss: 0.2967 | Val Acc: 0.9046\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 3750/3750 [01:23<00:00, 44.84batch/s]\n",
            "Validation: 100%|██████████| 238/238 [00:01<00:00, 207.53batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5 | Train Loss: 0.1302 | Train Acc: 0.9559 | Val Loss: 0.2644 | Val Acc: 0.9153\n",
            "\n",
            "Restored model to the best state based on validation loss.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dapatkan y_true dan label prediksi"
      ],
      "metadata": {
        "id": "OLShxc-3jws7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true, y_pred = model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZFB8NHXidUm",
        "outputId": "3ff904c5-2578-4e33-904a-7d49ce7ac91f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 238/238 [00:00<00:00, 332.98batch/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hitung Accuracy, Precision, Recall, F1, and Cofusion Matrix"
      ],
      "metadata": {
        "id": "PnLsrEtnj3LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Akurasi: {accuracy:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNo3F_-PigDr",
        "outputId": "a337ce11-028a-47e5-d2a5-9bd1676a9a82"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9155\n",
            "Recall: 0.9153\n",
            "F1 Score: 0.9147\n",
            "Akurasi: 0.9153\n",
            "[[1788   37   36   39]\n",
            " [  28 1864    4    4]\n",
            " [ 119   26 1602  153]\n",
            " [  74   25   99 1702]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Prediksi"
      ],
      "metadata": {
        "id": "C0nK94w2j6YX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is text\"\n",
        "pred = model.predict(text) # or\n",
        "pred = model(text)\n",
        "print(pred) # text label in int format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHJ2nZiIjR2e",
        "outputId": "9a33e55b-acaf-4788-db5c-9891c52f36c8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class = pred.item()  # Mengonversi tensor menjadi nilai numerik\n",
        "print(predicted_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE2GCnxj2aaS",
        "outputId": "a99ec94b-c6e7-4638-cba9-c1cc1ce7764c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh daftar label\n",
        "class_labels = [\"negative\", \"neutral\", \"positive\"]\n",
        "\n",
        "# Ambil prediksi sebagai teks\n",
        "predicted_label = class_labels[predicted_class]\n",
        "print(predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFT78l_R6SGr",
        "outputId": "7b521c6f-4e16-4354-d83f-da72beff0521"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "negative\n"
          ]
        }
      ]
    }
  ]
}